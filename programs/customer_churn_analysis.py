# -*- coding: utf-8 -*-
"""Customer_Churn_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o3gItLO7Vkwz68TqysgUU-KBkS708Fql

#Exploratory Data Analysis
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
# %matplotlib inline

dataset = pd.read_csv("../../../../../Downloads/Customer_churn.csv")
print(dataset.shape)
dataset.head()

dataset.dtypes

dataset.describe()

plt.xlabel("Count")
plt.ylabel("Target Variable")
dataset["Churn"].value_counts().plot(kind="barh", figsize=(8, 6))

100*dataset['Churn'].value_counts()/len(dataset['Churn'])

dataset.info(verbose = True)

empty_data = pd.DataFrame((dataset.isnull().sum()*100/dataset.shape[0]).reset_index())
plt.figure(figsize=(16, 5))
ax = sns.pointplot(x='index', y=0, data=empty_data)
plt.xticks(rotation='vertical', fontsize=7)
plt.title('Percentage')
plt.ylabel('Percentage of missing values')
plt.show()

"""#Data Cleaning"""

df = dataset.copy()

df.TotalCharges = pd.to_numeric(df.TotalCharges, errors='coerce')
df.isnull().sum()

df.loc[df['TotalCharges'].isnull() == True]

df.dropna(how='any', inplace = True)

max_tenure_value = df['tenure'].max()

labels = ["{0} - {1}".format(i, i + 11) for i in range(1, max_tenure_value, 12)]
df['tenure_group'] = pd.cut(df.tenure, range(1, max_tenure_value + 10, 12), right=False, labels=labels)

df['tenure_group'].value_counts()

df.drop(columns = ['customerID', 'tenure'], inplace = True)
df.head()

"""#Data Exploration

##Univariate Analysis
"""

for i, predictor in enumerate(df.drop(columns = ['Churn', 'TotalCharges', 'MonthlyCharges'])):
    plt.figure(i)
    sns.countplot(data = df, x = predictor, hue = 'Churn')

df['Churn'] = np.where(df['Churn'] == 'Yes', 1, 0)

df_dummies = pd.get_dummies(df)
df_dummies.head()

sns.lmplot(data = df_dummies, x = 'MonthlyCharges', y = 'TotalCharges', fit_reg = False)

Mth = sns.kdeplot(df_dummies.MonthlyCharges[(df_dummies["Churn"] == 0) ], color="Red", fill = True)
Mth = sns.kdeplot(df_dummies.MonthlyCharges[(df_dummies["Churn"] == 1) ], color="Blue", fill = True)

Mth.legend(["Not Churn","Churn"],loc='upper right')
Mth.set_ylabel('Density')
Mth.set_xlabel('Monthly Charges')
Mth.set_title('Monthly charges by churn')

Mth = sns.kdeplot(df_dummies.TotalCharges[(df_dummies["Churn"] == 0) ], color="Red", fill = True)
Mth = sns.kdeplot(df_dummies.TotalCharges[(df_dummies["Churn"] == 1) ], color="Blue", fill = True)

Mth.legend(["Not Churn","Churn"],loc='upper right')
Mth.set_ylabel('Density')
Mth.set_xlabel('Total Charges')
Mth.set_title('Total  charges by churn')

plt.figure(figsize=(25, 8))
df_dummies.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')

plt.figure(figsize=(20, 20))
sns.heatmap(df_dummies.corr(), cmap="Paired")

"""#Bivariate Analysis"""

df_churn = df.loc[df['Churn'] == 1]
df_not_churn = df.loc[df['Churn'] == 0]

df_churn['Partner'].value_counts()

def uniplot(df,col,title,hue=None):
  sns.set_style('whitegrid')
  sns.set_context('talk')
  plt.rcParams["axes.labelsize"] = 20
  plt.rcParams['axes.titlesize'] = 22
  plt.rcParams['axes.titlepad'] = 30

  temp = pd.Series(data = hue)
  fig, ax = plt.subplots()
  width = len(df[col].unique()) + 7 + 4*len(temp.unique())
  fig.set_size_inches(width , 8)
  plt.xticks(rotation=45)
  plt.title(title)
  ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index,hue = hue,palette='bright')

  for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')

  plt.show()

uniplot(df_churn, col='Partner', title='Distribution of Gender for Churned Customers', hue='gender')

uniplot(df_not_churn, col='Partner', title='Distribution of Gender for Non Churned Customers',hue='gender')

uniplot(df_churn,col='PaymentMethod',title='Distribution of PaymentMethod for Churned Customers',hue='gender')

uniplot(df_churn,col='Contract',title='Distribution of PaymentMethod for Churned Customers',hue='gender')

uniplot(df_churn,col='TechSupport',title='Distribution of TechSupport for Churned Customers',hue='gender')

uniplot(df_churn,col='SeniorCitizen',title='Distribution of SeniorCitizen for Churned Customers',hue='gender')

df_dummies.to_csv('Customer_churn_dummies.csv', index=False)

"""#Model Building"""

import pandas as pd
from sklearn import metrics
import pickle
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from imblearn.combine import SMOTEENN

df = pd.read_csv('../../../../../Downloads/Customer_churn_dummies.csv')
df.head(5)

X = df.drop('Churn', axis=1)
y = df['Churn']

print(X.columns)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

"""##Decision Tree Classifier"""

model_dt = DecisionTreeClassifier(criterion='entropy', random_state=100, max_depth=6, min_samples_leaf=8)

model_dt.fit(X_train, y_train)

y_pred = model_dt.predict(X_test)
y_pred

model_dt.score(X_test, y_test)

print(classification_report(y_test, y_pred, labels=[0, 1]))

print(confusion_matrix(y_test, y_pred))

sm = SMOTEENN()
X_resampled, y_resampled = sm.fit_resample(X, y)

xr_train, xr_test, yr_train, yr_test = train_test_split(X_resampled, y_resampled, test_size=0.2)

model_dt_smote = DecisionTreeClassifier(criterion='entropy', random_state=100, max_depth=6, min_samples_leaf=8)

model_dt_smote.fit(xr_train, yr_train)
yr_predict = model_dt_smote.predict(xr_test)
model_score_r = model_dt_smote.score(xr_test, yr_test)

print(confusion_matrix(yr_test, yr_predict))
print(model_score_r)

print(metrics.classification_report(yr_test, yr_predict))

"""##Random Forest Classifier"""

model_dt = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=100, max_depth=6, min_samples_leaf=8)
model_dt.fit(X_train, y_train)
y_pred = model_dt.predict(X_test)

print(confusion_matrix(y_test, y_pred))
model_dt.score(X_test, y_test)

print(classification_report(y_test, y_pred, labels=[0, 1]))

sm = SMOTEENN()
X_resampled, y_resampled = sm.fit_resample(X, y)

xr_train, xr_test, yr_train, yr_test = train_test_split(X_resampled, y_resampled, test_size=0.2)

model_dt_smote = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=100, max_depth=6, min_samples_leaf=8)
model_dt_smote.fit(xr_train, yr_train)

y_pred_smote = model_dt_smote.predict(xr_test)

print(confusion_matrix(yr_test, y_pred_smote))
print(model_dt_smote.score(xr_test, yr_test))

print(classification_report(yr_test, y_pred_smote, labels=[0, 1]))

filename = 'model.sav'

pickle.dump(model_dt_smote, open(filename, 'wb'))

"""#Reduction Dimensionality"""

from sklearn.decomposition import PCA
pca = PCA(0.9)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

xr_train_1 = pca.fit_transform(X_train, y_train)
xr_test_1 = pca.transform(X_test)

model_pca = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=100, max_depth=6, min_samples_leaf=8)
model_pca.fit(xr_train_1, y_train)

y_pred_pca = model_pca.predict(xr_test_1)

print(confusion_matrix(y_test, y_pred_pca))
print(model_pca.score(xr_test_1, y_test))

print(classification_report(y_test, y_pred_pca, labels=[0, 1]))

"""#Testing New Data"""

dt_t = pd.read_csv("../../../../../Downloads/first_telc.csv")
dt_t.drop(columns = ['Unnamed: 0'], inplace = True)
dt_t

labels = ["{0} - {1}".format(i, i + 11) for i in range(1, max_tenure_value, 12)]
dt_t['tenure_group'] = pd.cut(dt_t.tenure, range(1, max_tenure_value + 10, 12), right=False, labels=labels)
dt_t.drop(columns = ['tenure'], inplace = True)

categorical_columns = [
    'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',
    'MultipleLines', 'InternetService', 'OnlineSecurity',
    'OnlineBackup', 'DeviceProtection', 'TechSupport',
    'StreamingTV', 'StreamingMovies', 'Contract',
    'PaperlessBilling', 'PaymentMethod', 'tenure_group'
]

# Crear columnas dummies para las variables categóricas
dt_dummies = pd.get_dummies(dt_t[categorical_columns])

# Separar las columnas no categóricas
dt_non_categorical = dt_t.drop(columns=categorical_columns)

# Concatenar las columnas dummies con las no categóricas
final_data = pd.concat([dt_dummies, dt_non_categorical], axis=1)

# Cambiar el orden de las columnas
final_data = final_data[[
    'SeniorCitizen', 'MonthlyCharges', 'TotalCharges', 'gender_Female',
    'gender_Male', 'Partner_No', 'Partner_Yes', 'Dependents_No',
    'Dependents_Yes', 'PhoneService_No', 'PhoneService_Yes',
    'MultipleLines_No', 'MultipleLines_No phone service',
    'MultipleLines_Yes', 'InternetService_DSL',
    'InternetService_Fiber optic', 'InternetService_No',
    'OnlineSecurity_No', 'OnlineSecurity_No internet service',
    'OnlineSecurity_Yes', 'OnlineBackup_No',
    'OnlineBackup_No internet service', 'OnlineBackup_Yes',
    'DeviceProtection_No', 'DeviceProtection_No internet service',
    'DeviceProtection_Yes', 'TechSupport_No',
    'TechSupport_No internet service', 'TechSupport_Yes', 'StreamingTV_No',
    'StreamingTV_No internet service', 'StreamingTV_Yes',
    'StreamingMovies_No', 'StreamingMovies_No internet service',
    'StreamingMovies_Yes', 'Contract_Month-to-month', 'Contract_One year',
    'Contract_Two year', 'PaperlessBilling_No', 'PaperlessBilling_Yes',
    'PaymentMethod_Bank transfer (automatic)',
    'PaymentMethod_Credit card (automatic)',
    'PaymentMethod_Electronic check', 'PaymentMethod_Mailed check',
    'tenure_group_1 - 12', 'tenure_group_13 - 24', 'tenure_group_25 - 36',
    'tenure_group_37 - 48', 'tenure_group_49 - 60', 'tenure_group_61 - 72'
]]

# Asegúrate de que final_data sea un DataFrame con las columnas correctas
y_pred_smote = model_dt_smote.predict(final_data.iloc[0:final_data.shape[1]])  # Selecciona la primera fila
print(y_pred_smote)